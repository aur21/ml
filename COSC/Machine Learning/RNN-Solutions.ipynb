{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "express-depth",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Programming Practice\n",
    "\n",
    "COSC 410: Applied Machine Learning\\\n",
    "Colgate University\\\n",
    "*Prof. Apthorpe*\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will give you practice with the following topics:\n",
    "  1. Creating and training simple RNNs\n",
    "  2. Creating and training LSTM & GRU networks\n",
    "\n",
    "We will be using a new **natural language** dataset of IMDB movie reviews. We will attempt to perform a classification task to label reviews as having either *positive* or *negative* sentiment. The reviews have already been preprocessed from words --> numbers, such that the most common word is encoded as 1, the second most common word is encoded as 2, etc. \n",
    "\n",
    "## Part 1. Data Import & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = ks.datasets.imdb.load_data(num_words=max_features) # Load the dataset\n",
    "\n",
    "x_train = ks.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) # Pad reviews with 0s to equal length\n",
    "x_val = ks.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen) # Pad reviews with 0s to equal length\n",
    "\n",
    "print(len(x_train), \"Training sequences\") # Print number of training reviews\n",
    "print(len(x_val), \"Validation sequences\") # Print number of validation reviews\n",
    "\n",
    "print(np.unique(y_train, return_counts=True)[1]) # print counts of classes in training set\n",
    "print(np.unique(y_val, return_counts=True)[1]) # print counts of classes in validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-valve",
   "metadata": {},
   "source": [
    "**Note:** One of the benefits of RNNs is that they can handle sequences of differing lengths, so why do we pad the training and validation data to equal lengths? ... Training time! Keras is optimized for mini-batch gradient descent when the training examples are non-ragged matrices (a.k.a. *tensors*...hence \"Tensorflow\"). Once we have trained the model, we can perform predictions on *new* reviews of arbitary length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts an embedded review back into English bag-of-words representation\n",
    "def reverse_embedding(review):\n",
    "    words_to_ints = ks.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")\n",
    "    reverse_embedding_map = dict(zip(words_to_ints.values(), words_to_ints.keys()))\n",
    "    return \" \".join([reverse_embedding_map.get(x, \"\") for x in review])\n",
    "\n",
    "# Function that prints the label (1 = positive review, 0 = negative review) and bag-of-words text of a review\n",
    "def print_example_and_label(idx):\n",
    "    print(f\"Label: {y_train[idx]}\")\n",
    "    print(reverse_embedding(x_train[idx]) + \"\\n\")\n",
    "    \n",
    "print_example_and_label(0)\n",
    "print_example_and_label(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-logan",
   "metadata": {},
   "source": [
    "## Part 2. 1-Node RNNs\n",
    "\n",
    "We'll start by creating three different 1-Node RNNs with `SimpleRNN`, `LSTM`, and `GRU` nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN model\n",
    "model = ks.Sequential([\n",
    "    ks.Input(shape=(None,)),                     # Input for variable-length sequences\n",
    "    ks.layers.Embedding(max_features, 1),        # Embed each integer in a 1-dimensional vector\n",
    "    ks.layers.SimpleRNN(1, activation=\"sigmoid\") # Single RNN node     \n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "model = ks.Sequential([\n",
    "    ks.Input(shape=(None,)),                   # Input for variable-length sequences\n",
    "    ks.layers.Embedding(max_features, 1),      # Embed each integer in a 1-dimensional vector\n",
    "    ks.layers.LSTM(1, activation=\"sigmoid\")    # Single LSTM node \n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU model\n",
    "model = ks.Sequential([\n",
    "    ks.Input(shape=(None,)),                   # Input for variable-length sequences\n",
    "    ks.layers.Embedding(max_features, 1),      # Embed each integer in a 64-dimensional vector\n",
    "    ks.layers.GRU(1, activation=\"sigmoid\")     # Single GRU node \n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-affiliation",
   "metadata": {},
   "source": [
    "## Part 3. Recurrent Layer Hyperparameters\n",
    "\n",
    "`SimpleRNN`, `LSTM` and `GRU` layers share several hyperparameters that can be set via keyword arguments. The documentation for each of these layers is here: https://keras.io/api/layers/recurrent_layers/\n",
    "  * `return_sequences`: Whether the layers should return a single value for each input sequence (`False`, *default*) or a new multi-step sequence for each input sequence (`True`)\n",
    "  * `kernel_regularizer`, `bias_regularizer`, and `recurrent_regularizer` for L1, L2, or L1L2 regularization\n",
    "  * `dropout` and `recurrent_dropout` for setting dropout rates of forward and recurrent connections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-reynolds",
   "metadata": {},
   "source": [
    "## Part 4: Stacked RNNs\n",
    "\n",
    "Finally, let's try a stacked LSTM network with 2 hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked LSTM model\n",
    "model = ks.Sequential([\n",
    "    ks.Input(shape=(None,)),                    # Input for variable-length sequences\n",
    "    ks.layers.Embedding(max_features, 1),      # Embed each integer in a 64-dimensional vector\n",
    "    ks.layers.LSTM(64, return_sequences=True),  # Add recurrent layers\n",
    "    ks.layers.LSTM(64, return_sequences=True), \n",
    "    ks.layers.LSTM(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-japanese",
   "metadata": {},
   "source": [
    "We can see that the stacked RNN has higher plateau performance and reaches that performance more quickly. However, the increasing difference between train accuracy and val accuracy indicates that we are in an overfitting regime, so it doesn't make sense to increase the size of the network further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-honduras",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
