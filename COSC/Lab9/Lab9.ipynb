{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elementary-marina",
   "metadata": {},
   "source": [
    "# Lab 9: Natural Language Processing\n",
    "COSC 410: Applied Machine Learning\\\n",
    "Colgate University\\\n",
    "*Prof. Apthorpe*\n",
    "\n",
    "This lab is due to Gradescope by the beginning of lab next week (2:45p on 4/7). You may work with a partner on this lab – if you do, submit only one solution as a “group” on Gradescope. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will implement a recurrent neural network to perform text generation.  The network you will create will perform **character-level forecasting**. Given a sequence of characters, the model will predict the next character in the sequence. When applied iteratively, this allows the model to generate new sequences of text. Note that the model will never be given specific instruction about English spelling, grammar, or other conventions. It will try to learn all of these things from the training input. \n",
    "\n",
    "We will be using plain text files as training data, starting with the Brothers Grimm fairytale \"Little Red-Cap\" (known in America as \"Little Red Riding Hood\").  This text is on the short end of the amount of training input needed to train a text generation model and may result in generated text that mimics entire passages of the input. However, a smaller input text dramatically reduces training time while still showing how the process works -- perfect for this lab exercise.\n",
    "\n",
    "## Provided Files\n",
    " * `Lab9.ipynb`: This file\n",
    " * `red_riding_hood.txt`: plaintext version of the Brothers Grimm fairytale \"Little Red-Cap\" \n",
    " \n",
    "## Part 1: Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blind-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-astronomy",
   "metadata": {},
   "source": [
    "Complete the `load_input` function, which should \n",
    "  1) load a `.txt` file into one (long) string\n",
    "  2) replace all '\\n' characters with ' ' (space) characters\n",
    "  3) convert all characters to lowercase\n",
    "  4) return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "agricultural-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(filename):\n",
    "    fileStr = open(filename, \"r\", encoding='utf8').read()\n",
    "    fileStr = fileStr.replace('\\n', ' ')\n",
    "    fileStr = fileStr.lower()\n",
    "    return fileStr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-leonard",
   "metadata": {},
   "source": [
    "RNNs can't operate on strings directly, so we need to convert the characters into integers.\n",
    "\n",
    "Complete the following functions to compute the **vocabulary** of the text (a list containing all the **unique** characters in the text), encode string texts into integer lists, and decode integer lists back to string texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "failing-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(text):\n",
    "   uniqChars = []\n",
    "   for i in range(0, len(text)):\n",
    "      isUniq = True\n",
    "      for j in range(0, len(uniqChars)):\n",
    "         if(text[i] == uniqChars[j] and i != j):\n",
    "            isUniq = False\n",
    "            break\n",
    "      if (isUniq):\n",
    "         uniqChars.append(text[i])\n",
    "   return uniqChars\n",
    "\n",
    "def encode(text, vocab):\n",
    "   encodedText = []\n",
    "   for i in range(0, len(text)):\n",
    "      encodedText.append(vocab.index(text[i]))\n",
    "   return encodedText\n",
    "\n",
    "def decode(tokens, vocab):\n",
    "   decodedList = []\n",
    "   for i in range(0, len(tokens)):\n",
    "      decodedList.append(vocab[tokens[i]])\n",
    "   decodedText = \"\".join(decodedList)\n",
    "   return decodedText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-continuity",
   "metadata": {},
   "source": [
    "Next we need to create training examples and training labels for our model. The goal of the model is to take a sequence of characters and predict what character should come next. Complete the following function to divide the text into overlapping *subsequences* of characters (training examples) and a list of the characters immediately after each subsequence (training labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "optical-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(tokens, seq_length):\n",
    "   sequences = []\n",
    "   next = []\n",
    "   for i in range(0, len(tokens) - seq_length):\n",
    "      seq = []\n",
    "      for j in range(i, i + seq_length):\n",
    "         seq.append(tokens[j])\n",
    "      sequences.append(seq)\n",
    "      next.append(tokens[i + seq_length])\n",
    "   return (sequences, next)\n",
    "\n",
    "   \"\"\"Divides tokens (list of integers) into overlapping subsequences of length seq_length.\n",
    "       Returns these subsequences as a list of lists, also returns a list with the \n",
    "       integer value immediately following each subsequence\n",
    "    \n",
    "       Example:\n",
    "          generate_sequences([0, 1, 2, 2, 3, 4, 5, 6, 3, 7, 2, 8], 4) -->\n",
    "              [[0, 1, 2, 2],\n",
    "               [1, 2, 2, 3],\n",
    "               [2, 2, 3, 4],\n",
    "               [2, 3, 4, 5],\n",
    "               [3, 4, 5, 6],\n",
    "               [4, 5, 6, 3],\n",
    "               [5, 6, 3, 7], \n",
    "               [6, 3, 7, 2]]]  (1st return value)\n",
    "               \n",
    "             [3, 4, 5, 6, 3, 7, 2, 8]  (2nd return value)\n",
    "       \n",
    "       The reference implementation is 6 LoC.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-configuration",
   "metadata": {},
   "source": [
    "If you have programmed the previous functions correctly, the following cell will run with no errors and produce the following output:\n",
    "```\n",
    "Length of input text (in characters): 7376\n",
    "Vocab size: 36\n",
    "Training examples shape: (7325, 50)\n",
    "Training labels shape: (7325,)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prescription-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text (in characters): 7376\n",
      "Vocab size: 36\n",
      "Training examples shape: (7326, 50)\n",
      "Training labels shape: (7326,)\n"
     ]
    }
   ],
   "source": [
    "text = load_input(\"red_riding_hood.txt\")\n",
    "vocab = get_vocab(text)\n",
    "tokens = encode(text, vocab)\n",
    "assert(decode(tokens, vocab) == text)\n",
    "\n",
    "seq_length = 50\n",
    "x, y = generate_sequences(tokens, seq_length)\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "print(f\"Length of input text (in characters): {len(text)}\")\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Training examples shape: {x.shape}\")\n",
    "print(f\"Training labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-wheat",
   "metadata": {},
   "source": [
    "## Part 2: RNN Creation & Training\n",
    "\n",
    "Complete the following function that creates and compiles an LSTM model for character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prime-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, rnn_units):\n",
    "\n",
    "   model = ks.Sequential([\n",
    "    ks.Input(shape=(None,)),\n",
    "    ks.layers.Embedding(vocab_size, embedding_dim),\n",
    "    ks.layers.LSTM(rnn_units),\n",
    "    ks.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "   model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "   return model\n",
    "\n",
    "   \"\"\"Creates, compiles, and returns a LSTM model for character prediction. The model should have \n",
    "       at least 3 layers: an Embedding layer, a LSTM layer, and a Dense layer. \n",
    "       The model should produce 1 prediction per input sequence (i.e. the next character following the sequence),\n",
    "       NOT 1 prediction per step of the sequence.\n",
    "       \n",
    "       Arguments:\n",
    "          vocab_size: number of unique characters accross all training examples, also the input size of the Embedding layer\n",
    "          embedding_dim: output size of Embedding layer\n",
    "          rnn_units: number of units in LSTM layer\n",
    "          \n",
    "       Use the \"adam\" optimizer for best performance.\n",
    "       \n",
    "       The reference implementation is 7 LoC using the Keras Sequential API\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-hacker",
   "metadata": {},
   "source": [
    "Complete the following function that takes a trained model and uses it to generate new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "written-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed, num_chars, vocab):\n",
    "   text = []\n",
    "   text.append(encode(seed, vocab))\n",
    "   for i in range(0, num_chars):\n",
    "      pred = model.predict(text)\n",
    "      newChar = encode(np.random.choice(vocab, p=pred[0]), vocab)\n",
    "      text[0].append(newChar[0]) \n",
    "   return decode(text[0], vocab)\n",
    "   \n",
    "\n",
    "   \"\"\"Iteratively runs model.predict() to generate successive characters starting from the characters in seed. \n",
    "       Each generated character is appended to the input of the following model.predict() call. \n",
    "       \n",
    "       Returns the generated text decoded back into a string.\n",
    "       \n",
    "       Remember that model.predict will return a probability distribution, not a single integer. \n",
    "       You will need to convert these probabilities into an integer by RANDOMLY SAMPLING an index\n",
    "       based on the distribution weights, NOT by using np.argmax (which can lead to repetitions in generated text)\n",
    "       \n",
    "       You will have to be careful with your array shapes. You will want to include print statements to inspect\n",
    "           the shapes of intermediate values to help with debugging.\n",
    "       \n",
    "       Arguments:\n",
    "          model: trained model\n",
    "          seed: string with \"starter\" seed for text generation. This will need to be encoded before it is used in model.predict\n",
    "          num_chars: the number of characters that should be generated\n",
    "          vocab: list of unique characters in all training examples\n",
    "       \n",
    "       The reference implementation is 7 LoC\n",
    "    \"\"\"\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-bookmark",
   "metadata": {},
   "source": [
    "To test the `create_model` and `generate_text` functions, the following cell creates a model and uses it to generate 10 characters *untrained*. This will produce gibberish, but will let you know whether there are runtime errors you need to fix before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dental-admission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apveh’;dwrpy.ftngl!yu\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "seed = \"a\"\n",
    "num_chars_to_generate = 20\n",
    "\n",
    "model = create_model(len(vocab), embedding_dim, rnn_units)\n",
    "\n",
    "generated_text = generate_text(model, seed, num_chars_to_generate, vocab)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-outside",
   "metadata": {},
   "source": [
    "Once you have the previous cell working, it is time to train! The following two cells create and train a model, printing some example generated text after each epoch. You can stop and resume the training at any point by interrupting the kernel and then re-running the cell that calls `model.fit`. As the training progresses, you will hopefully see the generated text looking more and more like English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "growing-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "seed = \"a\"\n",
    "num_chars_to_generate = 100\n",
    "\n",
    "generate_text_callback = ks.callbacks.LambdaCallback(on_epoch_end=lambda epoch, log: print(generate_text(model, seed, num_chars_to_generate, vocab)))\n",
    "model = create_model(len(vocab), embedding_dim, rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "weighted-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0592 - accuracy: 0.4113tired no lfangssang. st cand ingren the shot’ tho sot fa see doadc, but uth, ig bunnutha ber th crasshe cured!.’, the fut read the drattther, she dup.’  ‘‘t to lakl inge ‘tll ingm tre blattthe puld the bo \n",
      "58/58 [==============================] - 37s 646ms/step - loss: 2.0592 - accuracy: 0.4113\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.8930 - accuracy: 0.4500tired at o then igtever wire hou-l,.’  ‘a fatley yo the cand on a watit cnring the sore snand fhe bit griy mous ond oo frices ren and theno you mil; apdad sohe ited wou f. bothegre watly geand whe wout sof\n",
      "58/58 [==============================] - 39s 667ms/step - loss: 1.8930 - accuracy: 0.4500\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7649 - accuracy: 0.4850tired of cher bet led gar. het rap cath athe, sou han ts poke she sand whoce: and the grand op mas noth.’  woos mevoks whoch doun; wiovimouss the waud sho wo ilf she at it’ som.’ soo and she car prean low \n",
      "58/58 [==============================] - 39s 667ms/step - loss: 1.7649 - accuracy: 0.4850\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.6415 - accuracy: 0.5121tired to of a lirlo and denek late dit no pas are thin i wer pemt inttle rayd rcanker ta renins yout iw is ek, sand aneveryus, aad camk to gar in fittle, to cind ore and s‘ip, bis caraed the wot in se whit\n",
      "58/58 [==============================] - 38s 657ms/step - loss: 1.6415 - accuracy: 0.5121\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5149 - accuracy: 0.5449tired to snevenom fhes is the woised it hhs and oo herfray, he faved she dort trat she and spepetticel; he wall, faw bechelf bet roun ti githy thate ag your ht home, she waid bewaning of aroud the lost, ‘w\n",
      "58/58 [==============================] - 38s 658ms/step - loss: 1.5149 - accuracy: 0.5449\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3850 - accuracy: 0.5800tired she butt.  little hed caw nother by the lolf fam of?’  ‘bey, and what the hiice oy, ‘whilk doung tood to sal, and beentan tout ghandmotherw tis serely but not ule that seaved, to she goof ol, little \n",
      "58/58 [==============================] - 38s 655ms/step - loss: 1.3850 - accuracy: 0.5800\n",
      "Epoch 7/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2516 - accuracy: 0.6188tired dopn fark fray of no, you sat the walk little ceday nor, she walk non ure sower! hav yhish nfre; to i mence, and ncake open the moos, what owerd had she wellasse scickky ary certing you wine’ rey hid\n",
      "58/58 [==============================] - 38s 655ms/step - loss: 1.2516 - accuracy: 0.6188\n",
      "Epoch 8/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1073 - accuracy: 0.6596tired: ‘ap, and! brevever, dease, you pase, and revevoured hick nous i samrie, fill ame doring: veave  itait it. the reafee, and than strand pask lifge-cape, and.’ said, howned, ande looftet, and thenen so\n",
      "58/58 [==============================] - 38s 662ms/step - loss: 1.1073 - accuracy: 0.6596\n",
      "Epoch 9/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.9537 - accuracy: 0.7093tired ap bittle will entile the grttere st home in the would so dear.  you mear hinget out, in the  olinge sake and weak into the rundmother the moulf was a thite ter shom. walk beaved reang rean to fare o\n",
      "58/58 [==============================] - 38s 655ms/step - loss: 0.9537 - accuracy: 0.7093\n",
      "Epoch 10/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8020 - accuracy: 0.7619tired -too, grandmother with he cay pide to he grandmother, what peeped dicked, and was nother grandmother, ald devouthe, the fall, ‘note lat of red you waning resp med, do were were he ligcter on fet rede\n",
      "58/58 [==============================] - 38s 653ms/step - loss: 0.8020 - accuracy: 0.7619\n",
      "Epoch 11/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.8210tired and hen he sad caide ‘othe red-cap, in to one seen, ho falk no gorang wolf fitly hive.  she gay that she sou will ngave in over cakes any her at has bed fave hor i sich ay heive, and to the high, and\n",
      "58/58 [==============================] - 38s 661ms/step - loss: 0.6422 - accuracy: 0.8210\n",
      "Epoch 12/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.8701tired he laoted her goond ho wood pore: ot crouther is hers so she sun amser great care,’ said little red-cap, however, quicklad bye saud geat, so carely dos dave had cartelf to gut your goly.’ sale and ta\n",
      "58/58 [==============================] - 38s 654ms/step - loss: 0.4940 - accuracy: 0.8701\n",
      "Epoch 13/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.9117tired a reobed bittly chel douse the cath.’ red-cap, how prettore gire, and then well sarped buttle red-cap, however, what beagie yerting, and then he saw the grond tre lattle ded-cap reed then the ho she \n",
      "58/58 [==============================] - 38s 652ms/step - loss: 0.3808 - accuracy: 0.9117\n",
      "Epoch 14/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9481tired that she saw eat; to her.  she was surrisf id craid to the vere, so sthaight sor apred be wole bef untorer i ware alyakg way any were good. set into et the grandmother was quitel which suited her so \n",
      "58/58 [==============================] - 38s 655ms/step - loss: 0.2662 - accuracy: 0.9481\n",
      "Epoch 15/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9697tired to her, and trong to her, lot little red-cap, and told her grandmother ise that bege how farceas you quit, replap ear himbed in the wolf’s snailsed quit lett fith grand on, and when he was better to \n",
      "58/58 [==============================] - 38s 652ms/step - loss: 0.1887 - accuracy: 0.9697\n",
      "Epoch 16/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9816tired them to your grandmother came out alive also,  ald dever the wolf knocked, and cried: ‘open the door.’  ‘lift the latcf, and dees house fares her her frong on prey loud.  ‘the better to eat to heg yo\n",
      "58/58 [==============================] - 38s 651ms/step - loss: 0.1318 - accuracy: 0.9816\n",
      "Epoch 17/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9858tired ‘what a terryible big mouth you have!’  ‘the better to hear you with, myof will,’ was the  grandmother’s sellidg ather to ary, and then he made two snips merthered whea waese that the wolf wage and w\n",
      "58/58 [==============================] - 38s 655ms/step - loss: 0.0954 - accuracy: 0.9858\n",
      "Epoch 18/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9887tired little red-cap.’  tome dear! how unely but mers downer again he woul not her guard, and went straight forward her a loett felfw olay the wood, that he may not come in.’ soon afterwather to ear yuring\n",
      "58/58 [==============================] - 38s 656ms/step - loss: 0.0733 - accuracy: 0.9887\n",
      "Epoch 19/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9915tired the wolf’ said her geand tto her so weroight to her cary that he collapsed at once, and fell dead.  then he saw st alse frringht here in her wants anything to hommes, and when she wentang that she we\n",
      "58/58 [==============================] - 38s 656ms/step - loss: 0.0619 - accuracy: 0.9915\n",
      "Epoch 20/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9928tired the wolf’; and after that the aged grandmother came ou her guard, and went straight forward on, when he mad made two snep curt a liftill be aw the bedt, but the slied of the cake or ath intt the soo \n",
      "58/58 [==============================] - 38s 655ms/step - loss: 0.0491 - accuracy: 0.9928\n",
      "Epoch 21/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9945tired juin her you with.’  ‘oh! but, get-at in her wood, when he made two snips, he saw the little red-cap shining, and then he made two sis, i whill the rutf, red-cap, however, quichly fetchided the wolf \n",
      "58/58 [==============================] - 38s 664ms/step - loss: 0.0386 - accuracy: 0.9945\n",
      "Epoch 22/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9962tired you have!’  ‘the better to eat you with!’  and scarcely had the wolf said this, than with one bound he was out of bed and swallowed up red-cap.  when the wolf had appeased his appetite, he lay sto ga\n",
      "58/58 [==============================] - 38s 659ms/step - loss: 0.0289 - accuracy: 0.9962\n",
      "Epoch 23/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9971tired ‘coke in the wood, when my mother has forbidden me to do so.’   it also related that once when red-cap went into the room, she had such a strange feeling that she said to herself: ‘oh dear! how uneas\n",
      "58/58 [==============================] - 38s 660ms/step - loss: 0.0253 - accuracy: 0.9971\n",
      "Epoch 24/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9986tired ‘wely, and just as little red-cap entered the wood, a wolf met her.  got dar you stine were so theave you with, my dear.’  ‘the went joutt your pry snought in the wolf said this, than wis one bound h\n",
      "58/58 [==============================] - 38s 657ms/step - loss: 0.0184 - accuracy: 0.9986\n",
      "Epoch 25/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9984tired ‘o oo, he say into the wood womai still get chel. ore at last scuack of if the sautager and went her. red-cap did not know what a wicked creature he was, and was not at all afraid of him.  ‘good day,\n",
      "58/58 [==============================] - 38s 656ms/step - loss: 0.0162 - accuracy: 0.9984\n",
      "Epoch 26/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9993tired ‘coke, and at last jumped on the roof, intending to wait until red-cap went home in the evening, and then to steal after her and devour her in the darkness. but the grandmother saw what was in his th\n",
      "58/58 [==============================] - 38s 656ms/step - loss: 0.0128 - accuracy: 0.9993\n",
      "Epoch 27/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9993tired the wolf’; and af latt strechired ohe suinto the grandmother saw what was in his thoughts. in front of the house was a great stone trough, so she said to the child: ‘take the pail, red-cap; i made  t\n",
      "58/58 [==============================] - 38s 655ms/step - loss: 0.0121 - accuracy: 0.9993\n",
      "Epoch 28/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9990tired and so got deeper and deeper into the wood.  meanwhile the wolf rans thare it oo the grandmother, ‘i am little red-cap, and am bringing you some cakes.’ but they did not speak, or open the door, so t\n",
      "58/58 [==============================] - 38s 660ms/step - loss: 0.0110 - accuracy: 0.9990\n",
      "Epoch 29/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9992tired the wolf’s belly, and when he awoke, he wanted to run away, but the stones were so heavy that he collapsed at once, and fell dead.  then all three were delighted. the huntsman drew off the wolf’s ski\n",
      "58/58 [==============================] - 38s 657ms/step - loss: 0.0104 - accuracy: 0.9992\n",
      "Epoch 30/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9993tired and sento rut of the ilf tone away her so early in the dey rand other is to have something good, to make her stronger.’  ‘where does your grandmother live, little red-cap?’  ‘a good quarter of a leag\n",
      "58/58 [==============================] - 38s 655ms/step - loss: 0.0080 - accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1145a7d60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=[generate_text_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-portland",
   "metadata": {},
   "source": [
    "Finally, experiment with the trained model in the following cell to see how different seeds affect the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beneficial-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tired-wher winegesat heanthed anith oredgon ‘bertolke, ther yofr, thily, and rtrer’ ind. sther rethocmen stheo  fouug dourerd ‘unicuthe, indind pint ther sf lid;r’ whed ‘nand thermas theund ‘the betry sit \n"
     ]
    }
   ],
   "source": [
    "# seed = \"little-red\"\n",
    "# seed = \"hood\"\n",
    "# seed = \"wolf\"\n",
    "seed = \"tired\"\n",
    "num_chars_to_generate = 200\n",
    "\n",
    "generated_text = generate_text(model, seed, num_chars_to_generate, vocab)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-basket",
   "metadata": {},
   "source": [
    "## Part 3: Questions\n",
    "\n",
    "**Question 1:** This model performs *character-level* forecasting. Another approach would be to perform *word-level* forecasting, where the model takes a sequence of words and predicts the next word in the sequence. In the following cell, discuss the pros and cons of character-level vs. word-level text generation. What are 2 reasons why character-level forecasting might be preferable. What are two reasons why word-level forecasting might be preferable?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "verified-spirituality",
   "metadata": {},
   "source": [
    "- Character level forecasting ensures that we can concretely define the size of the output layer without limiting all unique words that can appear in the output; the output layer's size is simply the number of characters that can appear in the output text. Whereas with word-level forecasting, there are too many words to all include in the output layer, so all words that can appear in the output would have to be defined in the architecture, which would restrict the words that the model can produce.\n",
    "- Character level forecasting is less computationally expensive because it does not have to loop through the characters of words to encode, decode and generate the vocab\n",
    "- Word level forecasting ensures that the model outputs intact and legible words without having to wait after a certain number of epochs, because the words that the model can output are already defined in the architecture.\n",
    "- Because word level forecasting produces intact words out of the box, the model does not need to learn how to create legible words, and only needs to learn how to arrange those words into valid sentences (grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-residence",
   "metadata": {},
   "source": [
    "**Question 2:** The model you created was not given any specific instruction about English words, English grammar, or anything else related to the language other than the sequence of characters in the example text. What elements of proper English do you see emerging in the text generated after each training epoch? How many epochs does it take for these to appear? What does the model still struggle with?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "advisory-volume",
   "metadata": {},
   "source": [
    "The model started producing somewhat proper quotations and commas around epoch 7, even though most words are still illegible. Around epoch 20, the model was able to produce more legible words than non-legible ones.\n",
    "The model by the end still did not produce sentences that are not in the training set, and the model could not learn grammar; any signs of grammar are only replicated from the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
